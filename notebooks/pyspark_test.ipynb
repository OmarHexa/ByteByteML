{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"practise\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = spark.read.option('header','True').csv('../Data/netflix_titles.csv',inferSchema=True)\n",
    "data = spark.read.csv(\"books.csv\", inferSchema=True, header=True)\n",
    "# data.printSchema(5)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select([\"book_id\", \"title\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_1 = (17, \"Imaginary\", \"unknown\", \"unknown\", 2022, 100, None)\n",
    "new_row_2 = (18, \"Impossible\", None, None, 2022, 50, None)\n",
    "\n",
    "# Append the new row to the DataFrame\n",
    "df = data.union(spark.createDataFrame([new_row_1, new_row_2], schema=data.schema))\n",
    "\n",
    "# Display the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"book_id\", \"id\")\n",
    "df = df.withColumn(\"new_stock\", df[\"stock_quantity\"] + 100)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"new_stock\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping nan values\n",
    "# df.na.drop (how='any',subset=['author_fname']).show()\n",
    "# filling nan values\n",
    "df.na.fill(\"Mising Values\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.filter((df['stock_quantity']<=100) & (df['released_year']>2000)).select(['title','stock_quantity','released_year']).show()\n",
    "# df.groupBy('released_year').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get 5 books with the largest page count\n",
    "\"\"\"\n",
    "SELECT book_id, title, pages FROM books\n",
    "ORDER BY pages DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "result = df.orderBy(df.pages.desc()).select([\"id\", \"title\", \"pages\"]).head(5)\n",
    "spark.createDataFrame(result).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most recent 5 published books with page count more than 100\n",
    "\"\"\"\n",
    "SELECT title, released_year, pages FROM books\n",
    "WHERE pages>100\n",
    "ORDER BY released_year DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "result = (\n",
    "    df.filter(df.pages > 100)\n",
    "    .orderBy(df.released_year.desc())\n",
    "    .select([\"title\", \"released_year\", \"pages\"])\n",
    "    .head(5)\n",
    ")\n",
    "spark.createDataFrame(result).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group each authors work and count their total number of pages published.\n",
    "\"\"\"\n",
    "SELECT author_fname, author_lname, SUM(pages) AS total_pages FROM books\n",
    "GROUP BY author_fname, author_lname\n",
    "ORDER BY total_pages DESC;\n",
    "\"\"\"\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by authors and calculate the total number of pages published\n",
    "result_df = (\n",
    "    df.groupBy(\"author_fname\", \"author_lname\")\n",
    "    .agg(F.sum(\"pages\").alias(\"total_pages\"))\n",
    "    .orderBy(\"total_pages\", ascending=False)\n",
    ")\n",
    "# Display the result DataFrame\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the condition\n",
    "result_df = df.filter(\n",
    "    (F.length(\"title\") - F.length(F.regexp_replace(\"title\", \" \", \"\")) + 1) > 2\n",
    ").select(\"id\", \"title\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get environment variables\n",
    "DATABASE_URL = os.getenv(\"JDBC_URL\")\n",
    "USER = os.getenv(\"USER\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "\n",
    "# Define connection properties\n",
    "properties = {\"user\": USER, \"password\": PASSWORD, \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "service = \"amazon\"\n",
    "# Specify the table name\n",
    "table_name = \"(SELECT type FROM {service}) AS newTable\"\n",
    "\n",
    "# Read data from the MySQL table into a DataFrame\n",
    "dataFrame = spark.read.jdbc(url=DATABASE_URL, table=table_name, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = \"netflix\"\n",
    "query = f\"(SELECT show_id,listed_in FROM {service}) AS newTable\"\n",
    "df_genres = spark.read.jdbc(\n",
    "    url=DATABASE_URL,\n",
    "    table=query,\n",
    "    properties=properties,\n",
    ")\n",
    "df_genres.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Split genres, count occurrences, and create DataFrame\n",
    "genre_counts = (\n",
    "    df_genres.select(F.explode(F.split(F.col(\"listed_in\"), \", \")).alias(\"genre\"))\n",
    "    .groupBy(\"genre\")\n",
    "    .count()\n",
    "    .sort(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "genre_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = \"netflix\"\n",
    "query = f\"(SELECT country FROM {service}) AS newTable\"\n",
    "\n",
    "country_df = spark.read.jdbc(\n",
    "    url=DATABASE_URL,\n",
    "    table=query,\n",
    "    properties=properties,\n",
    ")\n",
    "country_df = country_df.filter(F.col(\"country\") != \"unknown\")\n",
    "# Split the 'country' column, explode, and count occurrences\n",
    "\n",
    "top_countries = (\n",
    "    country_df.withColumn(\"country\", F.explode(F.split(\"country\", \", \")))\n",
    "    .groupBy(\"country\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "    .limit(20)\n",
    ")\n",
    "\n",
    "top_countries.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
