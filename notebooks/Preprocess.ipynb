{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import all the neccsery module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following cell for configuration only. Here we use logging for printing instead of print statement becasue the printing can be disabled by setting to different log level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "# Set up a logger\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all the raw data files in memory for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW = \"../Data\"\n",
    "# Log all available datasets\n",
    "logger.info(\"All the available datasets: %s\", os.listdir(PATH_RAW))\n",
    "\n",
    "datasets_names = [filename for filename in os.listdir(PATH_RAW) if filename.endswith(\".csv\")]\n",
    "all_data = []\n",
    "for dir_ in datasets_names:\n",
    "    read_pd = pd.read_csv(os.path.join(PATH_RAW, dir_))\n",
    "    # read_pd[\"channel_streaming\"] = dir_.split(\"_\")[0]\n",
    "    all_data.append(read_pd)\n",
    "# Log all column names for each dataset\n",
    "for data in all_data:\n",
    "    logger.info(\"Columns in %s Data: \\n %s\", data[\"channel_streaming\"].iloc[0], data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in all_data:\n",
    "    nan_columns = data.columns[data.isnull().any()]\n",
    "    nan_columns_with_percentage = {\n",
    "        column: data[column].isnull().mean() * 100 for column in nan_columns\n",
    "    }  # Dict comprehension\n",
    "\n",
    "    logger.info(\"NaN Columns and Percentages in %s Data:\", data[\"channel_streaming\"].iloc[0])\n",
    "    for column, percentage in nan_columns_with_percentage.items():\n",
    "        logger.info(\"\\t %s: %.2f%%\", column, percentage)\n",
    "    logger.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that only director, cast, country, date_added, rating and duration has NaN values inside each dataset. Let's fill all the NaN values with \"N/A\" into the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in each dataset\n",
    "for data in all_data:\n",
    "    data[\"cast\"].fillna(\"unknown\", inplace=True)\n",
    "    data[\"director\"].fillna(\"unknown\", inplace=True)\n",
    "    data[\"country\"].fillna(\"unknown\", inplace=True)\n",
    "    data[\"date_added\"].fillna(\"unknown\", inplace=True)\n",
    "    data[\"rating\"].fillna(\"unknown\", inplace=True)\n",
    "    data[\"rating\"] = data[\"rating\"].str.upper()  # Use str.upper() directly\n",
    "    data[\"duration\"].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all the unique ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_ratings = set()\n",
    "# for data in all_data:\n",
    "#     ratings = data[\"rating\"].unique()\n",
    "#     unique_ratings.update(ratings)\n",
    "\n",
    "# for ratings in unique_ratings:\n",
    "#     print(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save all the modified dataset into processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = os.path.join(PATH_RAW, \"processed\")\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for data in all_data:\n",
    "    channel_name = data[\"channel_streaming\"].iloc[0]\n",
    "\n",
    "    data = data.drop(columns=[\"channel_streaming\"])\n",
    "\n",
    "    output_file_path = os.path.join(output_directory, f\"{channel_name}.csv\")\n",
    "    data.to_csv(output_file_path, index=False)\n",
    "logger.info(\"Datasets saved into separate CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_RAW = \"../Data/processed/\"\n",
    "# # Log all available datasets\n",
    "# logger.info(\"All the available datasets: %s\", os.listdir(PATH_RAW))\n",
    "\n",
    "# datasets_names = [filename for filename in os.listdir(PATH_RAW) if filename.endswith(\".csv\")]\n",
    "# processed_data = []\n",
    "# for dir_ in datasets_names:\n",
    "#     read_pd = pd.read_csv(os.path.join(PATH_RAW, dir_))\n",
    "#     read_pd[\"channel_streaming\"] = dir_.split(\"_\")[0]\n",
    "#     processed_data.append(read_pd)\n",
    "# # Log all column names for each dataset\n",
    "# for data in processed_data:\n",
    "#     logger.info(\"Columns in %s Data: \\n %s\", data[\"channel_streaming\"].iloc[0], data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data[0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in processed_data:\n",
    "#     nan_columns = data.columns[data.isnull().any()]\n",
    "#     nan_columns_with_percentage = {\n",
    "#         column: data[column].isnull().mean() * 100 for column in nan_columns\n",
    "#     }  # Dict comprehension\n",
    "\n",
    "#     logger.info(\"NaN Columns and Percentages in %s Data:\", data[\"channel_streaming\"].iloc[0])\n",
    "#     for column, percentage in nan_columns_with_percentage.items():\n",
    "#         logger.info(\"\\t %s: %.2f%%\", column, percentage)\n",
    "#     logger.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we fill our database with the prosessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "# Get the parent directory\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "#  Load environment variables from the .env file in the parent directory\n",
    "dotenv_path = os.path.join(parent_directory, \".env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(DATABASE_URL)\n",
    "file_directory = os.path.join(PATH_RAW, \"processed\")\n",
    "datasets_names = [filename for filename in os.listdir(file_directory) if filename.endswith(\".csv\")]\n",
    "for dir_ in datasets_names:\n",
    "    service_data = pd.read_csv(os.path.join(file_directory, dir_))\n",
    "    table_name = dir_.split(\".\")[0]\n",
    "    service_data.to_sql(table_name, con=engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have all the tables uploaded on our database and ready to be used.\n",
    "Please check your database and see all the tables are uploaded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
